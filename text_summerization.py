# -*- coding: utf-8 -*-
"""text summerization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BZVkkLBcL98TlzA-QTtw8IU4xbIu4KBh
"""



"""Getting Started with OpenAI: Earnings Call Summaries"""

!pip install openai -q

import openai

from getpass import getpass
openai.api_key = getpass()

"""Calling OpenAI From Python"""

prompt = "I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \"Unknown\". Q: Who is the greatest investor of all time?\n\n"
engine = 'text-davinci-003'
response = openai.Completion.create(
  engine=engine,
  prompt=prompt,
  temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.
  max_tokens=140,
  top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top P’s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.
  frequency_penalty=0,
  presence_penalty=1
)

response

"""summarization"""

import requests

url = "https://gist.githubusercontent.com/hackingthemarkets/e664894b65b31cbe8993e02d25d26768/raw/618afe09d07979cc72911ce79634ab5d2cc19a54/nvidia-earnings-call.txt"
response = requests.get(url)
transcript = response.text

transcript

prompt = f"{transcript}\n\ntl;dr:"

prompt

response = openai.Completion.create(
    engine="text-davinci-003",
    prompt=prompt,
    temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.
    max_tokens=140,
    top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top P’s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.
    frequency_penalty=0,
    presence_penalty=1
)

"""chunking data"""

words = transcript.split(" ")

# show the first 20 words
words[:20]

"""divide our list of words into equal parts - I'm hardcoding the number 6 here to divide the text into 6 equal parts"""

import numpy as np

chunks = np.array_split(words, 6)

chunks

"""Let's summarize the first chunk, chunks[0]."""

sentences = ' '.join(list(chunks[0]))

sentences

prompt = f"{sentences}\n\ntl;dr:"

response = openai.Completion.create(
    engine="text-davinci-003",
    prompt=prompt,
    temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.
    max_tokens=140,
    top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top P’s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.
    frequency_penalty=0,
    presence_penalty=1
)

response_text = response["choices"][0]["text"]
response_text

"""This looks pretty good! Let's now loop through all of the chunks and save the responses in a list called summary_responses. At the end we'll join all of the responses into a single summary and check out the output."""

summary_responses = []

for chunk in chunks:

    sentences = ' '.join(list(chunk))

    prompt = f"{sentences}\n\ntl;dr:"

    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        temperature=0.3, # The temperature controls the randomness of the response, represented as a range from 0 to 1. A lower value of temperature means the API will respond with the first thing that the model sees; a higher value means the model evaluates possible responses that could fit into the context before spitting out the result.
        max_tokens=150,
        top_p=1, # Top P controls how many random results the model should consider for completion, as suggested by the temperature dial, thus determining the scope of randomness. Top P’s range is from 0 to 1. A lower value limits creativity, while a higher value expands its horizons.
        frequency_penalty=0,
        presence_penalty=1
    )

    response_text = response["choices"][0]["text"]
    summary_responses.append(response_text)

full_summary = "".join(summary_responses)

print("full summary")
print(full_summary)

